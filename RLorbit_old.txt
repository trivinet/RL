PENDING TASKS
    - FIX INTRO, ADD + INFO, EXPLAIN METHODS, THINGS USED, ETC
    - En la intro discutir lo de matlab vs Python y GVE vs campo vectorial
    - CHAPTER 5
    - CHAPTER 6
    - ABSTRACT correction
    - references
    - annex (code)
    - annex (stable_baselines3)

    - meter comparación con Matlab
    - anexo con modelo Matlab
    - corregir simulación para 2D
    - simulación ecc.


def _compute_reward_old(self, state):
      a, e, i, raan, argp, *_ = state
      goal = self.goal
      max_steps = int((self.final_t*24*60*60)/self.dt)
      # Normalize differences
      da = abs(a - goal[0]) / 1000         # km-scale
      de = abs(e - goal[1])
      di = abs(i - goal[2]) / np.pi        # normalize radians
      draan = abs(raan - goal[3]) / (2*np.pi)
      dargp = abs(argp - goal[4]) / (2*np.pi)

      [ka, ke, ki, kraan, kargp] = self.k_parameters

      # Orbital state reward (negative sum of normalized errors)
      orbital_reward = -(ka * da + ke * de + ki * di + kraan * draan + kargp * dargp)

      # Optional: penalize mass loss (reward conserving fuel)
      m = state[-1]
      mass_penalty = -0.0001 * (self.mass_initial - m)

      # Optional: penalize time (to encourage faster convergence)
      time_penalty = -0.001 * len(self.trajectory)

      # Goal band enforcement
      goal_band = 10 # 10 km tolerance
      goal_a_min = goal[0] - goal_band
      goal_a_max = goal[0] + goal_band

      if goal_a_min <= a <= goal_a_max:
        """ goal_reward = +10.0 * (1 - len(self.trajectory) / max_steps)  # bigger bonus if reached early """
        goal_reward = 50.0  # huge reward on success
        self.steps_in_goal += 1
      else:
        goal_reward = 0.0

        if hasattr(self, "steps_in_goal") and self.steps_in_goal > 0:
          goal_reward -= 5.0 # penalize for leaving the goal after reward
          self.steps_in_goal = 0 # reset

      # throttle_penalty = -0.2 * (1 - self.last_throttle) # penalize low throttle
      throttle_reward = +0.2* self.last_throttle # encourage thrust

      return orbital_reward + mass_penalty + time_penalty + goal_reward + throttle_reward #+ throttle_penalty



    def _compute_reward_2D(self, state):
        a, e, i, raan, argp, *_ = state
        goal = self.goal
        max_steps = int((self.final_t * 24 * 60 * 60) / self.dt)

        # Track only semimajor axis deviation
        da = abs(a - goal[0]) / 1000  # normalize on km scale
        orbital_reward = -100.0 * da**2   # strong penalty when far

        # Goal band bonus (within ±10 km of target)
        goal_band = 10
        goal_a_min = goal[0] - goal_band
        goal_a_max = goal[0] + goal_band

        if goal_a_min <= a <= goal_a_max:
            goal_reward = 100.0
            self.steps_in_goal += 1
        else:
            goal_reward = 0.0
            self.steps_in_goal = 0

        # Encourage using throttle (avoid passive policy)
        throttle_reward = +0.1 * self.last_throttle

        # No mass or time penalties for now
        return orbital_reward + goal_reward + throttle_reward



def _compute_reward_old_2(self, state):
      a, e, i, raan, argp, *_ = state
      goal = self.goal
      max_steps = int((10*24*60*60)/self.dt)
      # Normalize differences
      """ da = abs(a - goal[0]) / 1000         # km-scale
      de = abs(e - goal[1])
      di = abs(i - goal[2]) / np.pi        # normalize radians
      draan = abs(raan - goal[3]) / (2*np.pi)
      dargp = abs(argp - goal[4]) / (2*np.pi)

      [ka, ke, ki, kraan, kargp] = self.k_parameters

      # Orbital state reward (negative sum of normalized errors)
      orbital_reward = -(ka * da + ke * de + ki * di + kraan * draan + kargp * dargp) """

      da = abs(a - goal[0]) / 1000  # km-scale
      orbital_reward = -10.0 * da

      # Optional: penalize mass loss (reward conserving fuel)
      m = state[-1]
      """ mass_penalty = -0.0001 * (self.mass_initial - m) """

      # Optional: penalize time (to encourage faster convergence)
      time_penalty = -0.001 * len(self.trajectory)

      # Goal band enforcement
      goal_band = 10 # 10 km tolerance
      goal_a_min = goal[0] - goal_band
      goal_a_max = goal[0] + goal_band

      if goal_a_min <= a <= goal_a_max:
        """ goal_reward = +10.0 * (1 - len(self.trajectory) / max_steps)  # bigger bonus if reached early """
        goal_reward = 50.0  # huge reward on success
        self.steps_in_goal += 1
      else:
        goal_reward = 0.0

        if hasattr(self, "steps_in_goal") and self.steps_in_goal > 0:
          goal_reward -= 5.0 # penalize for leaving the goal after reward
          self.steps_in_goal = 0 # reset

      """ throttle_penalty = -0.2 * (1 - self.last_throttle) # penalize low throttle """
      throttle_reward = +0.1* self.last_throttle # encourage thrust

      if len(self.trajectory) % 20000 == 0:
        print(f"[STEP {len(self.trajectory)}] a={a:.2f}, da={da:.3f}, reward={orbital_reward:.2f}, time_penalty={time_penalty:.2f}")

      return orbital_reward + goal_reward + throttle_reward + time_penalty #+ mass_penalty + throttle_penalty